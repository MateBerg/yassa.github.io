<!DOCTYPE html>
<html>

<head>
    <meta charset="utf-8" />
    <title>Gradient Origin Networks</title>
    <meta content="A generative model that uses a SIREN neural network to represent datasets without requiring an encoder." name="description" />
    <meta content="Gradient Origin Networks" property="og:title" />
    <meta content="An implicit generative model that quickly learns a latent representation without an encoder" property="og:description" />
    <meta content="summary" name="twitter:card" />
    <meta content="width=device-width, initial-scale=1" name="viewport" />
    <link rel="stylesheet" href="style.css">
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css">
    <link rel="shortcut icon" href="logo.png">
    <script async src="https://www.googletagmanager.com/gtag/js?id=UA-90396229-1"></script>
    <script> window.dataLayer = window.dataLayer || []; function gtag() { dataLayer.push(arguments); } gtag('js', new Date()); gtag('config', 'UA-90396229-1'); </script>
</head>

<body>
    <div class="content">
        <h1 class="title">Gradient Origin Networks</h1>
        <div class="authors">
            <a href="https://www.dur.ac.uk/research/directory/staff/?mode=staff&id=18951"><strong>Sam Bond-Taylor</strong></a>* <a href="https://twitter.com/sambondtaylor"><i class="fa fa-twitter" aria-hidden="true"></i></a> &nbsp;
            <a href="https://cwkx.github.io"><strong>Chris G. Willcocks</strong></a>* <a href="https://twitter.com/cwkx"><i class="fa fa-twitter" aria-hidden="true"></i></a>
            <br>
            <small>*Authors contributed equally</small>
            <br>
            <div>
                <a href="https://www.dur.ac.uk/computer.science/"><img style="vertical-align:middle; width:auto; height: 24px;" src="data/durham.png"><span style=""> Durham University</span></a>
            </div>
        </div>
        <div class="info">
            <p><strong>Abstract</strong> This paper proposes a new type of implicit generative model that is able to quickly learn a latent representation without an explicit encoder. This is achieved with an implicit neural network that takes as inputs points in the coordinate space alongside a latent vector initialised with zeros. The gradients of the data fitting loss with respect to this zero vector are jointly optimised to act as latent points that capture the data manifold. The results show similar characteristics to autoencoders, but with fewer parameters and the advantages of implicit representation networks.</p>
        </div>
        <div class="icons">
            <a href="https://arxiv.org/abs/2007.02798"><i class="fa fa-book" aria-hidden="true"></i><span style=""> arXiv</span></a>
            <a href="https://arxiv.org/pdf/2007.02798.pdf"><i class="fa fa-file-pdf-o" aria-hidden="true"></i><span style=""> Paper</span></a>
            <a href="https://github.com/cwkx/GON"><i class="fa fa-github" aria-hidden="true"></i><span style=""> Code</span></a>
            <a href="https://youtu.be/nQmLl0svkPo"><i class="fa fa-youtube" aria-hidden="true"></i><span style=""> YouTube</span></a>
            <a href="https://colab.research.google.com/gist/cwkx/8c3a8b514f3bdfe123edc3bb0e6b7eca/gon.ipynb"><i class="fa fa-google" aria-hidden="true"></i><span style=""> Colab</span></a>
            <a href="mailto:samuel.e.bond-taylor@durham.ac.uk,christopher.g.willcocks@durham.ac.uk"><i class="fa fa-envelope-o" aria-hidden="true"></i><span style=""> Contact</span></a>
            <a href="https://twitter.com/cwkx/status/1280471156508839936"><i class="fa fa-retweet" aria-hidden="true"></i><span style=""> Tweet</span></a>
        </div>
        <h2>Approach</h2>
        <p>
            Gradient Origin Networks (GONs) are comparable to Variational Autoencoders in that both compress data into latent representations and permit sampling in a single step. However, GONs use gradients as encodings meaning that they have a much simpler architecture:
        </p>
        <table class="spaced">
            <tr>
                <td><img style="vertical-align:middle; width:auto; height: 120px;" alt="Variational Autoencoder" src="data/vae.png"></td>
                <td><img style="vertical-align:middle; width:auto; height: 92.4px;" alt="Gradient Origin Network" src="data/gon.png"></td>
            </tr>
            <tr>
                <td>Variational Autoencoder</td>
                <td>Gradient Origin Network</td>
            </tr>
        </table>
        <p>
            In Gradient Origin Networks, unknown parameters in the latent space are initialised at the origin. This zero vector is concatenated with the known coordinate parameters, which becomes the inputs of an implicit representation network <i>F</i> with a prior over the gradients (a <a href="https://vsitzmann.github.io/siren/">SIREN</a> whose gradients are normally distributed). This means we can compute gradients of the data fitting loss with respect to these inputs, and jointly optimise for the new latents:
        </p>

        <img class="center" alt="Gradient Origin Network" src="data/gon-eqn.png">
        <p>At inference, the latent vector can be sampled in a single step without requiring iteration. Here are some random spherical interpolations showing this:</p>

        <table class="squashed" style="margin-top: 3ex; margin-bottom: 3ex;">
            <tr>
                <td> 
                    <video width="240" height="240" autoplay loop>
                        <source src="data/mnist-slerp.mp4" type="video/mp4">
                        Your browser does not support the video tag.
                    </video>
                </td>
                <td> 
                    <video width="240" height="240" autoplay loop>
                        <source src="data/fashion-slerp.mp4" type="video/mp4">
                        Your browser does not support the video tag.
                    </video>
                </td>
                <td>
                    <video width="240" height="240" autoplay loop>
                        <source src="data/coil-slerp.mp4" type="video/mp4">
                        Your browser does not support the video tag.
                    </video>
                </td>
            </tr>
            <tr>
                <td>MNIST</td>
                <td>FashionMNIST</td>
                <td>COIL20</td>
            </tr>
        </table>

        <h2>YouTube video</h2>

        <iframe width="100%" height="430" style="background-color: #FFFFFF" src="https://www.youtube.com/embed/nQmLl0svkPo?transparent=0" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>

        <p>In practice, we find GONs to be parameter-efficient and fast to train. For more details, please watch the YouTube video and read the paper in the links above.</p>

        <div class="info">
            <p><strong>Conclusion</strong> We have proposed a new type of implicit generative model that captures the dataset without requiring an explicit encoder. By initialising a latent vector of our unknown parameters with zeros, we have shown that it is possible to compute the gradients of the data fitting loss with respect to this origin, and then jointly fit the data while learning this new point of reference in the latent space. The results show this approach is able to represent datasets using a small number of parameters, with the advantage of implicit representation networks being able to model many different types of discrete signals.</p>
        </div>
        <h2>Citation</h2>
        <div class="bibtex">
            <pre><code>@article{bondtaylor2020gradient,
  title={Gradient Origin Networks},
  author={Sam Bond-Taylor and Chris G. Willcocks},
  journal={arXiv preprint arXiv:2007.02798},
  year={2020}
}</code></pre>
        </div>
    </div>
</body>